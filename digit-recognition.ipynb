{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Prediction with Convolutional Neural Network\n",
    "\n",
    "- MNIST dataset: is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n",
    "- Goal: build a simple artificial neural network to predict the digit in the images.\n",
    "- Reference: [Oddly Satisfying Deep Learning](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/cnn_over_mlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# for plotting data, loss, accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading mnist dataset from keras\n",
    "from keras import datasets\n",
    "\n",
    "# show progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for type hinting\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utils Functions\n",
    "\n",
    "1. **plot_data**: plot the random 8 images from the dataset.\n",
    "2. **Base Layer**: Base class for all the layers.\n",
    "3. **Activation Functions**: Linear, reLU, Sigmoid, Tanh, Softmax.\n",
    "3. **Weight Initialization**: Zeros, Ones, Random, Random Uniform.\n",
    "4. **Optimization Functions**: Gradient Descent, Stochastic Gradient Descent, RMSprop, Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(\n",
    "    X: np.ndarray, y: np.ndarray, y_proba: Optional[np.ndarray] = None\n",
    ") -> None:\n",
    "    nrows, ncols = 2, 4\n",
    "    _, axes = plt.subplots(nrows, ncols, figsize=(8, 4))\n",
    "\n",
    "    len_x = X.shape[0]\n",
    "    for idx in range(nrows * ncols):\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "        img_idx = np.random.randint(0, len_x)\n",
    "\n",
    "        ax.imshow(X[img_idx], cmap=\"gray\")\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "\n",
    "        true_label = f\"True: {y[img_idx]}\"\n",
    "        color = \"black\"\n",
    "\n",
    "        if y_proba is not None:\n",
    "            pred_label = f\"Pred: {y_proba[img_idx]}\"\n",
    "            color = \"green\" if y[img_idx] == y_proba[img_idx] else \"red\"\n",
    "\n",
    "        img_title = true_label if y_proba is None else f\"{true_label}\\n{pred_label}\"\n",
    "        ax.set_xlabel(img_title, color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Base Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "\n",
    "        TODO: return output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: gradient of loss with respect to output\n",
    "        :param lr: learning rate\n",
    "\n",
    "        TODO: update parameters and return input gradient\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_dimensions(self, inp_shape: tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"\n",
    "        :params inp_shape: shape of input\n",
    "\n",
    "        TODO: get the demensions of input and save it to self.m, self.Nc, self.Nh, self.Nw\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, lr: float, m: int, k: int):\n",
    "        \"\"\"\n",
    "        lr: learning rate\n",
    "        m: batch_size (sumber of samples in batch)\n",
    "        k: iteration_number\n",
    "\n",
    "        TODO: update parameters\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Activation Functions class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(BaseLayer):\n",
    "    def __init__(self, act: str = \"linear\") -> None:\n",
    "        \"\"\"\n",
    "        :param act: activation function's name (relu, sigmoid, tanh, linear)\n",
    "        \"\"\"\n",
    "        self.act = act\n",
    "\n",
    "    def linear(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    def d_linear(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones(x.shape)\n",
    "\n",
    "    def reLU(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def d_reLU(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0) * np.ones(x.shape)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def tanh(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def d_tanh(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 - self.tanh(x) ** 2\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        z = x - np.max(x, axis=-1, keepdims=True)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def d_softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        if len(x.shape) == 1:\n",
    "            x = np.array(x).reshape(1, -1)\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "\n",
    "        _, d = x.shape\n",
    "        a = self.softmax(x)\n",
    "        tensor1 = np.einsum(\"ij,ik->ijk\", a, a)\n",
    "        tensor2 = np.einsum(\"ij,jk->ijk\", a, np.eye(d, d))\n",
    "\n",
    "        return tensor2 - tensor1\n",
    "\n",
    "    def get_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data to apply activation function\n",
    "        \"\"\"\n",
    "        if self.act == \"linear\":\n",
    "            return self.linear(X)\n",
    "        elif self.act == \"reLU\":\n",
    "            return self.reLU(X)\n",
    "        elif self.act == \"sigmoid\":\n",
    "            return self.sigmoid(X)\n",
    "        elif self.act == \"tanh\":\n",
    "            return self.tanh(X)\n",
    "        elif self.act == \"softmax\":\n",
    "            return self.softmax(X)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid activation functions are linear, reLU, sigmoid, tanh, softmax\"\n",
    "            )\n",
    "\n",
    "    def get_d_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.act == \"linear\":\n",
    "            return self.d_linear(X)\n",
    "        elif self.act == \"reLU\":\n",
    "            return self.d_reLU(X)\n",
    "        elif self.act == \"sigmoid\":\n",
    "            return self.d_sigmoid(X)\n",
    "        elif self.act == \"tanh\":\n",
    "            return self.d_tanh(X)\n",
    "        elif self.act == \"softmax\":\n",
    "            return self.d_softmax(X)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid activation functions are linear, reLU, sigmoid, tanh, softmax\"\n",
    "            )\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data to apply activation function\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return self.get_activation(X)\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: gradient of loss with respect to output\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        f_prime = self.get_d_activation(self.X)\n",
    "\n",
    "        if self.activation_type == \"softmax\":\n",
    "            dx = np.einsum(\"ijk,ik->ij\", f_prime, dZ)\n",
    "        else:\n",
    "            dx = dZ * f_prime\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Weight Initialization class\n",
    "\n",
    "- Zeros initialization: $w = np.zeros(shape)$\n",
    "- Ones initialization: $w = np.ones(shape)$\n",
    "- Random initialization: $w = np.random.randn(shape)$\n",
    "- Random uniform initialization: $w = np.random.uniform(size=shape)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    def __init__(self, shape: tuple, init: str = \"random\", seed: int = 69) -> None:\n",
    "        \"\"\"\n",
    "        :param shape: shape of the weight matrix\n",
    "        :param init: type of initialization (available initializations: zeros, ones, random, random_uniform)\n",
    "        :param seed: seed for random initialization\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        self.init = init\n",
    "        self.seed = seed\n",
    "\n",
    "    def zeros(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.zeros(self.shape)\n",
    "\n",
    "    def ones(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.ones(self.shape)\n",
    "\n",
    "    def random(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.normal(size=self.shape)\n",
    "\n",
    "    def random_uniform(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.uniform(size=self.shape)\n",
    "\n",
    "    def get_initializer(self) -> np.ndarray:\n",
    "        if self.init == \"zeros\":\n",
    "            return self.zeros()\n",
    "        elif self.init == \"ones\":\n",
    "            return self.ones()\n",
    "        elif self.init == \"random\":\n",
    "            return self.random()\n",
    "        elif self.init == \"random_uniform\":\n",
    "            return self.random_uniform()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid initializations are: zeros, ones, random, random_uniform\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.  Optimizers class\n",
    "\n",
    "- Gradient Descent Optimizer: $w = w - \\alpha \\nabla_w L(w)$\n",
    "- Stochastic Gradient Descent Optimizer: $w = w - \\alpha \\nabla_w L(w)$\n",
    "- RMSprop Optimizer: $v = \\beta v + (1 - \\beta) \\nabla_w L(w) \\odot \\nabla_w L(w)$ and $w = w - \\alpha \\frac{\\nabla_w L(w)}{\\sqrt{v + \\epsilon}}$\n",
    "- Adam Optimizer: $m = \\beta_1 m + (1 - \\beta_1) \\nabla_w L(w)$, $v = \\beta_2 v + (1 - \\beta_2) \\nabla_w L(w) \\odot \\nabla_w L(w)$, $m_{\\text{corrected}} = \\frac{m}{1 - \\beta_1^t}$, $v_{\\text{corrected}} = \\frac{v}{1 - \\beta_2^t}$, and $w = w - \\alpha \\frac{m_{\\text{corrected}}}{\\sqrt{v_{\\text{corrected}} + \\epsilon}}$\n",
    "\n",
    "> Note: Actually, i only use the Gradient Descent Optimizer in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        op_type: str = \"GD\",\n",
    "        shape_W: tuple[int, int] = None,\n",
    "        shape_b: tuple[int, int] = None,\n",
    "        m1: float = 0.9,\n",
    "        m2: float = 0.999,\n",
    "        epsilon: int = 1e-8,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param op_type: type of optimizer (available optimizers: GD, SGD, RMSProp, Adam)\n",
    "        :param shape_W: shape of the weight matrix\n",
    "        :param shape_b: shape of the bias matrix\n",
    "        :param m1: hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Used in RMSprop\n",
    "        :param m2: hyperparameter for adam only\n",
    "        :param epsilon: parameter used in adam and RMSprop to prevent division by zero error\n",
    "        \"\"\"\n",
    "        self.op_type = op_type\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.vdW = np.zeros(shape_W)\n",
    "        self.vdb = np.zeros(shape_b)\n",
    "\n",
    "        self.SdW = np.zeros(shape_W)\n",
    "        self.Sdb = np.zeros(shape_b)\n",
    "\n",
    "    def GD(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param _: iteration number\n",
    "        \"\"\"\n",
    "        return dW, db\n",
    "\n",
    "    def SGD(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param _: iteration number\n",
    "        \"\"\"\n",
    "        self.vdW = self.m1 * self.vdW + (1 - self.m1) * dW\n",
    "        self.vdb = self.m1 * self.vdb + (1 - self.m1) * db\n",
    "\n",
    "        return self.vdW, self.vdb\n",
    "\n",
    "    def RMSProp(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        self.SdW = self.m2 * self.SdW + (1 - self.m2) * (dW**2)\n",
    "        self.Sdb = self.m2 * self.Sdb + (1 - self.m2) * (db**2)\n",
    "\n",
    "        den_W = np.sqrt(self.SdW) + self.epsilon\n",
    "        den_b = np.sqrt(self.Sdb) + self.epsilon\n",
    "\n",
    "        return dW / den_W, db / den_b\n",
    "\n",
    "    def Adam(self, dW: np.ndarray, db: np.ndarray, k: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        # momentum\n",
    "        self.vdW = self.m1 * self.vdW + (1 - self.m1) * dW\n",
    "        self.vdb = self.m1 * self.vdb + (1 - self.m1) * db\n",
    "\n",
    "        # rmsprop\n",
    "        self.SdW = self.m2 * self.SdW + (1 - self.m2) * (dW**2)\n",
    "        self.Sdb = self.m2 * self.Sdb + (1 - self.m2) * (db**2)\n",
    "\n",
    "        # correction\n",
    "        if k > 1:\n",
    "            vdW_h = self.vdW / (1 - (self.m1**k))\n",
    "            vdb_h = self.vdb / (1 - (self.m1**k))\n",
    "            SdW_h = self.SdW / (1 - (self.m2**k))\n",
    "            Sdb_h = self.Sdb / (1 - (self.m2**k))\n",
    "        else:\n",
    "            vdW_h = self.vdW\n",
    "            vdb_h = self.vdb\n",
    "            SdW_h = self.SdW\n",
    "            Sdb_h = self.Sdb\n",
    "\n",
    "        den_W = np.sqrt(SdW_h) + self.epsilon\n",
    "        den_b = np.sqrt(Sdb_h) + self.epsilon\n",
    "\n",
    "        return vdW_h / den_W, vdb_h / den_b\n",
    "\n",
    "    def get_optimizer(self, dW: np.ndarray, db: np.ndarray, k: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        if self.op_type == \"GD\":\n",
    "            return self.GD(dW, db, k)\n",
    "        elif self.op_type == \"SGD\":\n",
    "            return self.SGD(dW, db, k)\n",
    "        elif self.op_type == \"RMSProp\":\n",
    "            return self.RMSProp(dW, db, k)\n",
    "        elif self.op_type == \"Adam\":\n",
    "            return self.Adam(dW, db, k)\n",
    "        else:\n",
    "            raise ValueError(\"Valid optiomizers are GD, SGD, RMSProp, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5. Loss Functions class\n",
    "\n",
    "- Mean Squared Error Loss: $L(y, \\hat{y}) = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "- Derivative of Mean Squared Error Loss: $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} = \\hat{y} - y$\n",
    "- Binary Cross Entropy Loss: $L(y, \\hat{y}) = - \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$\n",
    "- Derivative of Binary Cross Entropy Loss: $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} = - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, loss: str = \"mse\") -> None:\n",
    "        \"\"\"\n",
    "        :param loss: str, loss function (Available: mse, cross-entropy)\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "\n",
    "    # Mean Squared Error\n",
    "    def mse(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        return (1 / 2) * np.sum((np.linalg.norm(a - y, axis=1)) ** 2)\n",
    "\n",
    "    def d_mse(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        return a - y\n",
    "\n",
    "    # Binary Cross Entropy\n",
    "    def cross_entropy(\n",
    "        self, a: np.ndarray, y: np.ndarray, epsilon: float = 1e-12\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        a = np.clip(a, epsilon, 1.0 - epsilon)\n",
    "        return -np.sum(y * np.log(a))\n",
    "\n",
    "    def d_cross_entropy(\n",
    "        self, a: np.ndarray, y: np.ndarray, epsilon: float = 1e-12\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        a = np.clip(a, epsilon, 1.0 - epsilon)\n",
    "        return -y / a\n",
    "\n",
    "    def get_loss(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            return self.mse(a, y)\n",
    "        elif self.loss == \"cross-entropy\":\n",
    "            return self.cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid losses are mse, cross-entropy\")\n",
    "\n",
    "    def get_d_loss(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            return self.d_mse(a, y)\n",
    "        elif self.loss == \"bce\":\n",
    "            return self.d_bce()\n",
    "        else:\n",
    "            raise ValueError(\"Valid losses are mse, cross-entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6. Learning Rate Decay class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def constant(self, t: int, lr: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        \"\"\"\n",
    "        return lr\n",
    "\n",
    "    def time_decay(self, t: int, lr: float, k: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param k: decay rate\n",
    "        \"\"\"\n",
    "        return lr / (1 + (k * t))\n",
    "\n",
    "    def step_decay(self, t: int, lr: float, F: int, D: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param F: fractor value controlling the decay\n",
    "        :param D: \"Drop every\" iteration\n",
    "        \"\"\"\n",
    "        return lr * (F ** np.floor((1 + t) / D))\n",
    "\n",
    "    def exponential_decay(self, t: int, lr: float, k: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param k: decay rate\n",
    "        \"\"\"\n",
    "        return lr * np.exp(-k * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding2D(BaseLayer):\n",
    "    def __init__(self, p: Union[str, tuple[int, int]] = \"valid\") -> None:\n",
    "        \"\"\"\n",
    "        :param p: padding type (valid, same or tuple[int,int])\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "\n",
    "    def get_dimensions(\n",
    "        self, inp_shape: tuple[int, int], kernel_size: int, s: tuple[int, int] = (1, 1)\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        :param inp_shape: input shape (H,W)\n",
    "        :param kernel_size: kernel size\n",
    "        :param s: stride\n",
    "        \"\"\"\n",
    "        if len(inp_shape) == 4:\n",
    "            m, Nc, Nh, Nw = inp_shape\n",
    "        elif len(inp_shape) == 3:\n",
    "            Nc, Nh, Nw = inp_shape\n",
    "\n",
    "        Kh, Kw = kernel_size\n",
    "        Sh, Sw = s\n",
    "        p = self.p\n",
    "\n",
    "        if type(p) == int:\n",
    "            pt, pb = p, p\n",
    "            pl, pr = p, p\n",
    "        elif type(p) == tuple:\n",
    "            ph, pw = p\n",
    "            pt, pb = ph // 2, (ph + 1) // 2\n",
    "            pl, pr = pw // 2, (pw + 1) // 2\n",
    "        elif p == \"valid\":\n",
    "            pt, pb = 0, 0\n",
    "            pl, pr = 0, 0\n",
    "        elif p == \"same\":\n",
    "            # calculating how much padding is required in all 4 directions\n",
    "            # (top, bottom, left and right)\n",
    "            ph = (Sh - 1) * Nh + Kh - Sh\n",
    "            pw = (Sw - 1) * Nw + Kw - Sw\n",
    "\n",
    "            pt, pb = ph // 2, (ph + 1) // 2\n",
    "            pl, pr = pw // 2, (pw + 1) // 2\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Valid padding types are: valid, same or tuple\")\n",
    "\n",
    "        if len(inp_shape) == 4:\n",
    "            out_shape = (m, Nc, Nh + pt + pb, Nw + pl + pr)\n",
    "        elif len(inp_shape) == 3:\n",
    "            out_shape = (Nc, Nh + pt + pb, Nw + pl + pr)\n",
    "\n",
    "        return out_shape, (pt, pb, pl, pr)\n",
    "\n",
    "    def forward(\n",
    "        self, X: np.ndarray, kernel_size: int, s: tuple[int, int] = (1, 1)\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param kernel_size: kernel size\n",
    "        :param s: stride\n",
    "\n",
    "        :return X_pad: padded input data\n",
    "        \"\"\"\n",
    "\n",
    "        self.inp_shape = X.shape\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "\n",
    "        self.out_shape, (self.pt, self.pb, self.pl, self.pr) = self.get_dimensions(\n",
    "            self.inp_shape, kernel_size, s\n",
    "        )\n",
    "\n",
    "        zeros_r = np.zeros((m, Nc, Nh, self.pr))\n",
    "        zeros_l = np.zeros((m, Nc, Nh, self.pl))\n",
    "        zeros_t = np.zeros((m, Nc, self.pt, Nw + self.pl + self.pr))\n",
    "        zeros_b = np.zeros((m, Nc, self.pb, Nw + self.pl + self.pr))\n",
    "\n",
    "        X_pad = np.concatenate((X, zeros_r), axis=3)\n",
    "        X_pad = np.concatenate((zeros_l, X_pad), axis=3)\n",
    "        X_pad = np.concatenate((zeros_t, X_pad), axis=2)\n",
    "        X_pad = np.concatenate((X_pad, zeros_b), axis=2)\n",
    "\n",
    "        return X_pad\n",
    "\n",
    "    def backpropagation(self, dZ, lr):\n",
    "        \"\"\"\n",
    "        :param dZ: Backprop Error of padded X (Xp)\n",
    "\n",
    "        :return dX: Backprop Error of X\n",
    "        \"\"\"\n",
    "        m, Nc, Nh, Nw = self.inp_shape\n",
    "        dX = dZ[:, :, self.pt : self.pt + Nh, self.pl : self.pl + Nw]\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(BaseLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        kernel_size: Union[int, tuple[int, int]] = 3,\n",
    "        s: tuple[int, int] = (1, 1),\n",
    "        p: Union[str, tuple[int, int]] = \"valid\",\n",
    "        act: str = \"linear\",\n",
    "        is_bias: bool = True,\n",
    "        weight_init: str = \"random\",\n",
    "        kernel_regularizer: tuple[str, float] = None,\n",
    "        seed: int = 69,\n",
    "        inp_shape: tuple[int, int, int, int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param filters: number of filters in the convolutional layer\n",
    "        :param kernel_size: size of the kernel, int or tuple of 2 integers (default: 3)\n",
    "        :param s: stride (Sh, Sw) (default: (1,1))\n",
    "        :param p: padding type, valid or same or tuple[int,int] (default: valid)\n",
    "        :param act: activation function, valid activations are linear, reLU, sigmoid, tanh, softmax (default: linear)\n",
    "        :bias: bool, whether to use bias in the convolutional layer\n",
    "        :param weight_init: weight initialization type, valid initializations are zeros, ones, random, random_uniform (default: random)\n",
    "        :param kernel_regularizer: kernel regularizer, valid regularizers are ('L2', 0.01) or ('L1', 2)\n",
    "        :param seed: seed to generate random values\n",
    "        :param inp_shape: input shape (m, Nc, Nh, Nw)\n",
    "        \"\"\"\n",
    "        self.padding = Padding2D(p=p)\n",
    "        self.F = filters\n",
    "        self.inp_shape_x = inp_shape\n",
    "\n",
    "        if type(kernel_size) == int:\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        elif type(kernel_size) == tuple and len(kernel_size) == 2:\n",
    "            self.kernel_size = kernel_size\n",
    "        self.Kh, self.Kw = self.kernel_size\n",
    "\n",
    "        if type(s) == int:\n",
    "            self.s = (s, s)\n",
    "        elif type(s) == tuple and len(s) == 2:\n",
    "            self.s = s\n",
    "        self.Sh, self.Sw = self.s\n",
    "\n",
    "        self.act = Activation(act=act)\n",
    "        self.is_bias = is_bias\n",
    "        self.weight_init = weight_init\n",
    "\n",
    "        if kernel_regularizer is None:\n",
    "            self.kernel_regularizer = (\"L2\", 0)\n",
    "        else:\n",
    "            self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "    def get_dimensions(self, inp_shape: tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"\n",
    "        :param inp_shape: input shape (m, Nc, Nh, Nw)\n",
    "        \"\"\"\n",
    "        self.inp_shape_x = inp_shape\n",
    "\n",
    "        self.inp_shape, _ = self.padding.get_dimensions(\n",
    "            self.inp_shape_x, self.kernel_size, self.s\n",
    "        )\n",
    "\n",
    "        if len(inp_shape) == 3:\n",
    "            self.Nc, self.Nh, self.Nw = self.inp_shape\n",
    "        elif len(inp_shape) == 4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = self.inp_shape\n",
    "\n",
    "        # Output dimensions\n",
    "        self.Oh = ((self.Nh - self.Kh) // self.Sh) + 1\n",
    "        self.Ow = ((self.Nw - self.Kw) // self.Sw) + 1\n",
    "\n",
    "        if len(inp_shape) == 3:\n",
    "            self.out_shape = (self.F, self.Oh, self.Ow)\n",
    "        elif len(inp_shape) == 4:\n",
    "            self.out_shape = (self.m, self.F, self.Oh, self.Ow)\n",
    "\n",
    "    def init_params(self, inp_shape: tuple[int, int, int, int], opt_type: str) -> None:\n",
    "        \"\"\"\n",
    "        :param inp_shape: input shape (m, Nc, Nh, Nw)\n",
    "        :param opt_type: optimizer type (GD, SGD, RMSProp, Adam)\n",
    "        \"\"\"\n",
    "        self.get_dimensions(inp_shape)\n",
    "        shape_b = (self.F, self.Oh, self.Ow)\n",
    "        shape_k = (self.F, self.Nc, self.Kh, self.Kw)\n",
    "\n",
    "        initializer = WeightInitializer(\n",
    "            shape=shape_k, init=self.weight_init, seed=self.seed\n",
    "        )\n",
    "        self.K = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape=shape_b)\n",
    "\n",
    "        self.optimizer = Optimizer(op_type=opt_type, shape_W=shape_k, shape_b=shape_b)\n",
    "\n",
    "    def dilate2D(self, X: np.ndarray, Dr: tuple[int, int] = (1, 1)) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param Dr: dilation rate (Dh, Dw)\n",
    "        \"\"\"\n",
    "\n",
    "        dh, dw = Dr\n",
    "        m, C, H, W = X.shape\n",
    "        Xd = np.insert(arr=X, obj=np.repeat(np.arange(1, W), dw - 1), values=0, axis=-1)\n",
    "        Xd = np.insert(\n",
    "            arr=Xd, obj=np.repeat(np.arange(1, H), dh - 1), values=0, axis=-2\n",
    "        )\n",
    "        return Xd\n",
    "\n",
    "    def prepare_subMatrix(\n",
    "        self, X: np.ndarray, Kh: int, Kw: int, s: tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param Kh: kernel height\n",
    "        :param Kw: kernel width\n",
    "        :param s: stride (Sh, Sw)\n",
    "        \"\"\"\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "\n",
    "        Oh = (Nh - Kh) // sh + 1\n",
    "        Ow = (Nw - Kw) // sw + 1\n",
    "\n",
    "        strides = (Nc * Nh * Nw, Nw * Nh, Nw * sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(\n",
    "            X, shape=(m, Nc, Oh, Ow, Kh, Kw), strides=strides\n",
    "        )\n",
    "\n",
    "        return subM\n",
    "\n",
    "    def convolve(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        K: np.ndarray,\n",
    "        s: tuple[int, int] = (1, 1),\n",
    "        mode: str = \"front\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param K: kernel\n",
    "        :param s: stride (Sh, Sw)\n",
    "        :param mode: front or back or param\n",
    "        \"\"\"\n",
    "\n",
    "        F, Kc, Kh, Kw = K.shape\n",
    "        subM = self.prepare_subMatrix(X, Kh, Kw, s)\n",
    "\n",
    "        if mode == \"front\":\n",
    "            return np.einsum(\"fckl,mcijkl->mfij\", K, subM)\n",
    "        elif mode == \"back\":\n",
    "            return np.einsum(\"fdkl,mcijkl->mdij\", K, subM)\n",
    "        elif mode == \"param\":\n",
    "            return np.einsum(\"mfkl,mcijkl->fcij\", K, subM)\n",
    "\n",
    "    def dZ_D_dX(self, dZ_D: np.ndarray, Nh: int, Nw: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ_D: dilated dZ\n",
    "        :param Nh: input height\n",
    "        :param Nw: input width\n",
    "        \"\"\"\n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "\n",
    "        ph = Nh - Hd + self.Kh - 1\n",
    "        pw = Nw - Wd + self.Kw - 1\n",
    "\n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "\n",
    "        # Rotate K by 180 degrees\n",
    "\n",
    "        K_rotated = self.K[:, :, ::-1, ::-1]\n",
    "\n",
    "        # convolve dZ_Dp with K_rotated\n",
    "\n",
    "        dXp = self.convolve(dZ_Dp, K_rotated, mode=\"back\")\n",
    "\n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "\n",
    "        Xp = self.padding.forward(X, self.kernel_size, self.s)\n",
    "\n",
    "        # convolve Xp with K\n",
    "        Z = self.convolve(Xp, self.K, self.s) + self.b\n",
    "\n",
    "        a = self.act.forward(Z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backpropagation(self, dA: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dA: gradient of loss with respect to output\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        Xp = self.padding.forward(self.X, self.kernel_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZ = self.act.backpropagation(dA)\n",
    "\n",
    "        # Dilate dZ (dZ-> dZ_D)\n",
    "\n",
    "        dZ_D = self.dilate2D(dZ, Dr=self.s)\n",
    "\n",
    "        dX = self.dZ_D_dX(dZ_D, Nh, Nw)\n",
    "\n",
    "        # Gradient dK\n",
    "\n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "\n",
    "        ph = self.Nh - Hd - self.Kh + 1\n",
    "        pw = self.Nw - Wd - self.Kw + 1\n",
    "\n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "\n",
    "        self.dK = self.convolve(Xp, dZ_Dp, mode=\"param\")\n",
    "\n",
    "        # Gradient db\n",
    "        self.dB: np.ndarray = np.sum(dZ, axis=0)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr: float, m: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        lr: learning rate\n",
    "        m: batch_size (sumber of samples in batch)\n",
    "        k: iteration_number\n",
    "        \"\"\"\n",
    "        dK, dB = self.optimizer.get_optimizer(self.dK, self.dB, k)\n",
    "\n",
    "        if self.kernel_regularizer[0].lower() == \"l2\":\n",
    "            dK += self.kernel_regularizer[1] * self.K\n",
    "        elif self.weight_regularizer[0].lower() == \"l1\":\n",
    "            dK += self.kernel_regularizer[1] * np.sign(self.K)\n",
    "\n",
    "        self.K -= self.dK * (lr / m)\n",
    "\n",
    "        if self.is_bias:\n",
    "            self.b -= self.dB * (lr / m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling2D(BaseLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pool_size: Union[int, tuple[int, int]] = (2, 2),\n",
    "        s: Union[int, tuple[int, int]] = (2, 2),\n",
    "        p: Union[str, tuple[int, int]] = \"valid\",\n",
    "        pool_type: str = \"max\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param pool_size: size of the pooling window, int or tuple of 2 integers (default: (2,2))\n",
    "        :param s: stride, int or tuple of 2 integers (default: (2,2))\n",
    "        :param p: padding type, valid or same or tuple[int,int] (default: valid)\n",
    "        :param pool_type: type of pooling, max or mean (default: max)\n",
    "        \"\"\"\n",
    "        self.padding = Padding2D(p=p)\n",
    "\n",
    "        if type(pool_size) == int:\n",
    "            self.pool_size = (pool_size, pool_size)\n",
    "        elif type(pool_size) == tuple and len(pool_size) == 2:\n",
    "            self.pool_size = pool_size\n",
    "\n",
    "        self.Kh, self.Kw = self.pool_size\n",
    "\n",
    "        if type(s) == int:\n",
    "            self.s = (s, s)\n",
    "        elif type(s) == tuple and len(s) == 2:\n",
    "            self.s = s\n",
    "\n",
    "        self.sh, self.sw = self.s\n",
    "\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def get_dimensions(self, inp_shape: tuple[int, int, int, int]) -> None:\n",
    "        if len(inp_shape) == 4:\n",
    "            m, Nc, Nh, Nw = inp_shape\n",
    "        elif len(inp_shape) == 3:\n",
    "            Nc, Nh, Nw = inp_shape\n",
    "\n",
    "        Oh = (Nh - self.Kh) // self.sh + 1\n",
    "        Ow = (Nw - self.Kw) // self.sw + 1\n",
    "\n",
    "        if len(inp_shape) == 4:\n",
    "            self.out_shape = (m, Nc, Oh, Ow)\n",
    "        elif len(inp_shape) == 3:\n",
    "            self.out_shape = (Nc, Oh, Ow)\n",
    "\n",
    "    def prepare_subMatrix(\n",
    "        self, X: np.ndarray, pool_size: tuple[int, int], s: tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param pool_size: size of the pooling window\n",
    "        :param s: stride (Sh, Sw)\n",
    "        \"\"\"\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "        Kh, Kw = pool_size\n",
    "\n",
    "        Oh = (Nh - Kh) // sh + 1\n",
    "        Ow = (Nw - Kw) // sw + 1\n",
    "\n",
    "        strides = (Nc * Nh * Nw, Nh * Nw, Nw * sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(\n",
    "            X, shape=(m, Nc, Oh, Ow, Kh, Kw), strides=strides\n",
    "        )\n",
    "        return subM\n",
    "\n",
    "    def pooling(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        pool_size: tuple[int, int] = (2, 2),\n",
    "        s: tuple[int, int] = (2, 2),\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param pool_size: size of the pooling window\n",
    "        :param s: stride (Sh, Sw)\n",
    "        \"\"\"\n",
    "\n",
    "        subM = self.prepare_subMatrix(X, pool_size, s)\n",
    "\n",
    "        if self.pool_type == \"max\":\n",
    "            return np.max(subM, axis=(-2, -1))\n",
    "        elif self.pool_type == \"mean\":\n",
    "            return np.mean(subM, axis=(-2, -1))\n",
    "        else:\n",
    "            raise ValueError(\"Allowed pool types are only 'max' or 'mean'.\")\n",
    "\n",
    "    def prepare_mask(self, subM: np.ndarray, Kh: int, Kw: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param subM: submatrix\n",
    "        :param Kh: kernel height\n",
    "        :param Kw: kernel width\n",
    "        \"\"\"\n",
    "\n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "\n",
    "        a = subM.reshape(-1, Kh * Kw)\n",
    "        idx = np.argmax(a, axis=1)\n",
    "        b = np.zeros(a.shape)\n",
    "        b[np.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.reshape((m, Nc, Oh, Ow, Kh, Kw))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(\n",
    "        self, mask: np.ndarray, Xp: np.ndarray, dZ: np.ndarray, Kh: int, Kw: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param mask: mask\n",
    "        :param Xp: padded input data\n",
    "        :param dZ: Output Error\n",
    "        :param Kh: kernel height\n",
    "        :param Kw: kernel width\n",
    "        \"\"\"\n",
    "        dA = np.einsum(\"i,ijk->ijk\", dZ.reshape(-1), mask.reshape(-1, Kh, Kw)).reshape(\n",
    "            mask.shape\n",
    "        )\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "        strides = (Nc * Nh * Nw, Nh * Nw, Nw, 1)\n",
    "        strides = tuple(i * Xp.itemsize for i in strides)\n",
    "        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "\n",
    "    def maxpool_backprop(self, dZ: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: Output Error\n",
    "        :param X: input data\n",
    "        \"\"\"\n",
    "\n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        subM = self.prepare_subMatrix(Xp, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, Kh, Kw)\n",
    "\n",
    "        dXp = self.mask_dXp(mask, Xp, dZ, Kh, Kw)\n",
    "\n",
    "        return dXp\n",
    "\n",
    "    def dZ_dZp(self, dZ: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: Output Error\n",
    "        \"\"\"\n",
    "        sh, sw = self.s\n",
    "        Kh, Kw = self.pool_size\n",
    "\n",
    "        dZp = np.kron(dZ, np.ones((Kh, Kw), dtype=dZ.dtype))\n",
    "\n",
    "        jh, jw = Kh - sh, Kw - sw  # jump along height and width\n",
    "\n",
    "        if jw != 0:\n",
    "            L = dZp.shape[-1] - 1\n",
    "\n",
    "            l1 = np.arange(sw, L)\n",
    "            l2 = np.arange(sw + jw, L + jw)\n",
    "\n",
    "            mask = np.tile([True] * jw + [False] * jw, len(l1) // jw).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[: len(l1)]]\n",
    "            r2 = l2[mask[: len(l2)]]\n",
    "\n",
    "            dZp[:, :, :, r1] += dZp[:, :, :, r2]\n",
    "            dZp = np.delete(dZp, r2, axis=-1)\n",
    "\n",
    "        if jh != 0:\n",
    "            L = dZp.shape[-2] - 1\n",
    "\n",
    "            l1 = np.arange(sh, L)\n",
    "            l2 = np.arange(sh + jh, L + jh)\n",
    "\n",
    "            mask = np.tile([True] * jh + [False] * jh, len(l1) // jh).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[: len(l1)]]\n",
    "            r2 = l2[mask[: len(l2)]]\n",
    "\n",
    "            dZp[:, :, r1, :] += dZp[:, :, r2, :]\n",
    "            dZp = np.delete(dZp, r2, axis=-2)\n",
    "\n",
    "        return dZp\n",
    "\n",
    "    def averagepool_backprop(self, dZ: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: Output Error\n",
    "        :param X: input data\n",
    "        \"\"\"\n",
    "\n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZp = self.dZ_dZp(dZ)\n",
    "\n",
    "        ph = Nh - dZp.shape[-2]\n",
    "        pw = Nw - dZp.shape[-1]\n",
    "\n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dXp = padding_back.forward(dZp, s=self.s, kernel_size=self.pool_size)\n",
    "\n",
    "        return dXp / (Nh * Nw)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "\n",
    "        # padding\n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        Z = self.pooling(Xp, self.pool_size, self.s)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backpropagation(self, dZ, lr):\n",
    "        \"\"\"\n",
    "        :param dZ: Output Error\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        if self.pool_type == \"max\":\n",
    "            dXp = self.maxpool_backprop(dZ, self.X)\n",
    "        elif self.pool_type == \"mean\":\n",
    "            dXp = self.averagepool_backprop(dZ, self.X)\n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(BaseLayer):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.m, self.Nc, self.Nh, self.Nw = X.shape\n",
    "        X_flat = X.reshape((self.m, self.Nc * self.Nh * self.Nw))\n",
    "        return X_flat\n",
    "\n",
    "    def backpropagation(self, dZ, lr):\n",
    "        dX = dZ.reshape((self.m, self.Nc, self.Nh, self.Nw))\n",
    "        return dX\n",
    "\n",
    "    def get_dimensions(self, input_shape: tuple[int, int, int]):\n",
    "        if len(input_shape) == 4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.Nc, self.Nh, self.Nw = input_shape\n",
    "\n",
    "        self.output_shape = self.Nc * self.Nh * self.Nw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(BaseLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        neurons: int,\n",
    "        act: str = \"linear\",\n",
    "        is_bias: bool = True,\n",
    "        weight_init: str = \"random\",\n",
    "        weight_regularizer: tuple[str, float] = (\"L2\", 0),\n",
    "        seed: int = 69,\n",
    "        inp_dim: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param neurons: number of neurons in the dense layer\n",
    "        :param act: activation function, valid activations are linear, reLU, sigmoid, tanh, softmax (default: linear)\n",
    "        :param is_bias: bool, whether to use bias in the dense layer\n",
    "        :param weight_init: weight initialization type, valid initializations are zeros, ones, random, random_uniform (default: random)\n",
    "        :param weight_regularizer: weight regularizer, valid regularizers are ('L2', 0.01) or ('L1', 2)\n",
    "        :param seed: seed to generate random values\n",
    "        :param inp_dim: input dimension\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.act = Activation(act=act)\n",
    "        self.is_bias = is_bias\n",
    "        self.weight_init = weight_init\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.seed = seed\n",
    "        self.inp_dim = inp_dim\n",
    "\n",
    "    def init_params(self, hl: int, op_type: str) -> None:\n",
    "        \"\"\"\n",
    "        :param hl: number of neurons in the previous layer\n",
    "        \"\"\"\n",
    "        shape_W = (hl, self.neurons)\n",
    "        shape_b = (self.neurons, 1)\n",
    "        initializer = WeightInitializer(\n",
    "            shape=shape_W, init=self.weight_init, seed=self.seed\n",
    "        )\n",
    "\n",
    "        self.W = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape=shape_b)\n",
    "\n",
    "        self.optimizer = Optimizer(op_type=op_type, shape_W=shape_W, shape_b=shape_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.net = X @ self.W + self.b.T\n",
    "        return self.act.forward(self.net)\n",
    "\n",
    "    def backpropagation(self, dZ, lr):\n",
    "        dA: np.ndarray = self.act.backpropagation(dZ)\n",
    "        dR = dA.copy()\n",
    "        self.dB = np.sum(dA, axis=0).reshape(-1, 1)\n",
    "        self.dW = (self.X.T) @ dR\n",
    "        dX = dR @ (self.W.T)\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        dW, dB = self.optimizer.get_optimizer(self.dW, self.dB, k)\n",
    "\n",
    "        if self.weight_regularizer[0].lower() == \"l2\":\n",
    "            dW += self.weight_regularizer[1] * self.W\n",
    "        elif self.weight_regularizer[0].lower() == \"l1\":\n",
    "            dW += self.weight_regularizer[1] * np.sign(self.W)\n",
    "\n",
    "        self.W -= dW * (lr / m)\n",
    "\n",
    "        if self.is_bias:\n",
    "            self.b -= dB * (lr / m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5. Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(BaseLayer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        \"\"\"\n",
    "        p: Dropout probability\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        if self.p == 0:\n",
    "            self.p += 1e-6\n",
    "        if self.p == 1:\n",
    "            self.p -= 1e-6\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        Z = X * self.mask\n",
    "        return Z\n",
    "\n",
    "    def backpropagation(self, dZ, lr):\n",
    "        dX = dZ * self.mask\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6. CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, layers: list[BaseLayer] = None) -> None:\n",
    "        if layers is None:\n",
    "            self.layers = []\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "    def add(self, layer: BaseLayer) -> None:\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def Input(self, inp_shape: tuple[int, int, int, int]) -> None:\n",
    "        self.d = inp_shape\n",
    "        self.architecture = [self.d]\n",
    "        self.layer_name = [\"Input\"]\n",
    "\n",
    "    def create_architecture(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.__class__.__name__ == \"Conv2D\":\n",
    "                if layer.inp_shape_x is not None:\n",
    "                    self.Input(layer.inp_shape_x)\n",
    "                layer.get_dimensions(self.architecture[-1])\n",
    "                self.architecture.append(layer.out_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
