{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Prediction with Convolutional Neural Network\n",
    "\n",
    "- MNIST dataset: is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n",
    "- Goal: build a simple artificial neural network to predict the digit in the images.\n",
    "- Reference: [Oddly Satisfying Deep Learning](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/cnn_over_mlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# for plotting data, loss, accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading mnist dataset from keras\n",
    "from keras import datasets\n",
    "\n",
    "# show progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for type hinting\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utils Functions\n",
    "\n",
    "1. **plot_data**: plot the random 8 images from the dataset.\n",
    "2. **Base Layer**: Base class for all the layers.\n",
    "3. **Activation Functions**: Linear, reLU, Sigmoid, Tanh, Softmax.\n",
    "3. **Weight Initialization**: Zeros, Ones, Random, Random Uniform.\n",
    "4. **Optimization Functions**: Gradient Descent, Stochastic Gradient Descent, RMSprop, Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(\n",
    "    X: np.ndarray, y: np.ndarray, y_proba: Optional[np.ndarray] = None\n",
    ") -> None:\n",
    "    nrows, ncols = 2, 4\n",
    "    _, axes = plt.subplots(nrows, ncols, figsize=(8, 4))\n",
    "\n",
    "    len_x = X.shape[0]\n",
    "    for idx in range(nrows * ncols):\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "        img_idx = np.random.randint(0, len_x)\n",
    "\n",
    "        ax.imshow(X[img_idx], cmap=\"gray\")\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "\n",
    "        true_label = f\"True: {y[img_idx]}\"\n",
    "        color = \"black\"\n",
    "\n",
    "        if y_proba is not None:\n",
    "            pred_label = f\"Pred: {y_proba[img_idx]}\"\n",
    "            color = \"green\" if y[img_idx] == y_proba[img_idx] else \"red\"\n",
    "\n",
    "        img_title = true_label if y_proba is None else f\"{true_label}\\n{pred_label}\"\n",
    "        ax.set_xlabel(img_title, color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Base Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "\n",
    "        TODO: return output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: gradient of loss with respect to output\n",
    "        :param lr: learning rate\n",
    "\n",
    "        TODO: update parameters and return input gradient\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Activation Functions class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(BaseLayer):\n",
    "    def __init__(self, act: str = \"linear\") -> None:\n",
    "        \"\"\"\n",
    "        :param act: activation function's name (relu, sigmoid, tanh, linear)\n",
    "        \"\"\"\n",
    "        self.act = act\n",
    "\n",
    "    def linear(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    def d_linear(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones(x.shape)\n",
    "\n",
    "    def reLU(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def d_reLU(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0) * np.ones(x.shape)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def tanh(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def d_tanh(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 - self.tanh(x) ** 2\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        z = x - np.max(x, axis=-1, keepdims=True)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def d_softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        if len(x.shape) == 1:\n",
    "            x = np.array(x).reshape(1, -1)\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "\n",
    "        _, d = x.shape\n",
    "        a = self.softmax(x)\n",
    "        tensor1 = np.einsum(\"ij,ik->ijk\", a, a)\n",
    "        tensor2 = np.einsum(\"ij,jk->ijk\", a, np.eye(d, d))\n",
    "\n",
    "        return tensor2 - tensor1\n",
    "\n",
    "    def get_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data to apply activation function\n",
    "        \"\"\"\n",
    "        if self.act == \"linear\":\n",
    "            return self.linear(X)\n",
    "        elif self.act == \"reLU\":\n",
    "            return self.reLU(X)\n",
    "        elif self.act == \"sigmoid\":\n",
    "            return self.sigmoid(X)\n",
    "        elif self.act == \"tanh\":\n",
    "            return self.tanh(X)\n",
    "        elif self.act == \"softmax\":\n",
    "            return self.softmax(X)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid activation functions are linear, reLU, sigmoid, tanh, softmax\"\n",
    "            )\n",
    "\n",
    "    def get_d_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.act == \"linear\":\n",
    "            return self.d_linear(X)\n",
    "        elif self.act == \"reLU\":\n",
    "            return self.d_reLU(X)\n",
    "        elif self.act == \"sigmoid\":\n",
    "            return self.d_sigmoid(X)\n",
    "        elif self.act == \"tanh\":\n",
    "            return self.d_tanh(X)\n",
    "        elif self.act == \"softmax\":\n",
    "            return self.d_softmax(X)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid activation functions are linear, reLU, sigmoid, tanh, softmax\"\n",
    "            )\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data to apply activation function\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return self.get_activation(X)\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: gradient of loss with respect to output\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        f_prime = self.get_d_activation(self.X)\n",
    "\n",
    "        if self.activation_type == \"softmax\":\n",
    "            dx = np.einsum(\"ijk,ik->ij\", f_prime, dZ)\n",
    "        else:\n",
    "            dx = dZ * f_prime\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Weight Initialization class\n",
    "\n",
    "- Zeros initialization: $w = np.zeros(shape)$\n",
    "- Ones initialization: $w = np.ones(shape)$\n",
    "- Random initialization: $w = np.random.randn(shape)$\n",
    "- Random uniform initialization: $w = np.random.uniform(size=shape)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    def __init__(self, shape, init: str = \"random\", seed: int = 69) -> None:\n",
    "        \"\"\"\n",
    "        :param shape: shape of the weight matrix\n",
    "        :param init: type of initialization (available initializations: zeros, ones, random, random_uniform)\n",
    "        :param seed: seed for random initialization\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        self.init = init\n",
    "        self.seed = seed\n",
    "\n",
    "    def zeros(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.zeros(self.shape)\n",
    "\n",
    "    def ones(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.ones(self.shape)\n",
    "\n",
    "    def random(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.normal(size=self.shape)\n",
    "\n",
    "    def random_uniform(self) -> np.ndarray:\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.uniform(size=self.shape)\n",
    "\n",
    "    def get_initializer(self) -> np.ndarray:\n",
    "        if self.init == \"zeros\":\n",
    "            return self.zeros()\n",
    "        elif self.init == \"ones\":\n",
    "            return self.ones()\n",
    "        elif self.init == \"random\":\n",
    "            return self.random()\n",
    "        elif self.init == \"random_uniform\":\n",
    "            return self.random_uniform()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Valid initializations are: zeros, ones, random, random_uniform\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.  Optimizers class\n",
    "\n",
    "- Gradient Descent Optimizer: $w = w - \\alpha \\nabla_w L(w)$\n",
    "- Stochastic Gradient Descent Optimizer: $w = w - \\alpha \\nabla_w L(w)$\n",
    "- RMSprop Optimizer: $v = \\beta v + (1 - \\beta) \\nabla_w L(w) \\odot \\nabla_w L(w)$ and $w = w - \\alpha \\frac{\\nabla_w L(w)}{\\sqrt{v + \\epsilon}}$\n",
    "- Adam Optimizer: $m = \\beta_1 m + (1 - \\beta_1) \\nabla_w L(w)$, $v = \\beta_2 v + (1 - \\beta_2) \\nabla_w L(w) \\odot \\nabla_w L(w)$, $m_{\\text{corrected}} = \\frac{m}{1 - \\beta_1^t}$, $v_{\\text{corrected}} = \\frac{v}{1 - \\beta_2^t}$, and $w = w - \\alpha \\frac{m_{\\text{corrected}}}{\\sqrt{v_{\\text{corrected}} + \\epsilon}}$\n",
    "\n",
    "> Note: Actually, i only use the Gradient Descent Optimizer in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        op_type: str = \"GD\",\n",
    "        shape_W: tuple[int, int] = None,\n",
    "        shape_b: tuple[int, int] = None,\n",
    "        m1: float = 0.9,\n",
    "        m2: float = 0.999,\n",
    "        epsilon: int = 1e-8,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param op_type: type of optimizer (available optimizers: GD, SGD, RMSProp, Adam)\n",
    "        :param shape_W: shape of the weight matrix\n",
    "        :param shape_b: shape of the bias matrix\n",
    "        :param m1: hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. Used in RMSprop\n",
    "        :param m2: hyperparameter for adam only\n",
    "        :param epsilon: parameter used in adam and RMSprop to prevent division by zero error\n",
    "        \"\"\"\n",
    "        self.op_type = op_type\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.vdW = np.zeros(shape_W)\n",
    "        self.vdb = np.zeros(shape_b)\n",
    "\n",
    "        self.SdW = np.zeros(shape_W)\n",
    "        self.Sdb = np.zeros(shape_b)\n",
    "\n",
    "    def GD(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param _: iteration number\n",
    "        \"\"\"\n",
    "        return dW, db\n",
    "\n",
    "    def SGD(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param _: iteration number\n",
    "        \"\"\"\n",
    "        self.vdW = self.m1 * self.vdW + (1 - self.m1) * dW\n",
    "        self.vdb = self.m1 * self.vdb + (1 - self.m1) * db\n",
    "\n",
    "        return self.vdW, self.vdb\n",
    "\n",
    "    def RMSProp(self, dW: np.ndarray, db: np.ndarray, _: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        self.SdW = self.m2 * self.SdW + (1 - self.m2) * (dW**2)\n",
    "        self.Sdb = self.m2 * self.Sdb + (1 - self.m2) * (db**2)\n",
    "\n",
    "        den_W = np.sqrt(self.SdW) + self.epsilon\n",
    "        den_b = np.sqrt(self.Sdb) + self.epsilon\n",
    "\n",
    "        return dW / den_W, db / den_b\n",
    "\n",
    "    def Adam(self, dW: np.ndarray, db: np.ndarray, k: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        # momentum\n",
    "        self.vdW = self.m1 * self.vdW + (1 - self.m1) * dW\n",
    "        self.vdb = self.m1 * self.vdb + (1 - self.m1) * db\n",
    "\n",
    "        # rmsprop\n",
    "        self.SdW = self.m2 * self.SdW + (1 - self.m2) * (dW**2)\n",
    "        self.Sdb = self.m2 * self.Sdb + (1 - self.m2) * (db**2)\n",
    "\n",
    "        # correction\n",
    "        if k > 1:\n",
    "            vdW_h = self.vdW / (1 - (self.m1**k))\n",
    "            vdb_h = self.vdb / (1 - (self.m1**k))\n",
    "            SdW_h = self.SdW / (1 - (self.m2**k))\n",
    "            Sdb_h = self.Sdb / (1 - (self.m2**k))\n",
    "        else:\n",
    "            vdW_h = self.vdW\n",
    "            vdb_h = self.vdb\n",
    "            SdW_h = self.SdW\n",
    "            Sdb_h = self.Sdb\n",
    "\n",
    "        den_W = np.sqrt(SdW_h) + self.epsilon\n",
    "        den_b = np.sqrt(Sdb_h) + self.epsilon\n",
    "\n",
    "        return vdW_h / den_W, vdb_h / den_b\n",
    "\n",
    "    def get_optimizer(self, dW: np.ndarray, db: np.ndarray, k: int) -> tuple:\n",
    "        \"\"\"\n",
    "        :param dW: gradient of Weight W for iteration k\n",
    "        :param db: gradient of bias b for iteration k\n",
    "        :param k: iteration number\n",
    "        \"\"\"\n",
    "        if self.op_type == \"GD\":\n",
    "            return self.GD(dW, db, k)\n",
    "        elif self.op_type == \"SGD\":\n",
    "            return self.SGD(dW, db, k)\n",
    "        elif self.op_type == \"RMSProp\":\n",
    "            return self.RMSProp(dW, db, k)\n",
    "        elif self.op_type == \"Adam\":\n",
    "            return self.Adam(dW, db, k)\n",
    "        else:\n",
    "            raise ValueError(\"Valid optiomizers are GD, SGD, RMSProp, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5. Loss Functions class\n",
    "\n",
    "- Mean Squared Error Loss: $L(y, \\hat{y}) = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "- Derivative of Mean Squared Error Loss: $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} = \\hat{y} - y$\n",
    "- Binary Cross Entropy Loss: $L(y, \\hat{y}) = - \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$\n",
    "- Derivative of Binary Cross Entropy Loss: $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} = - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, loss: str = \"mse\") -> None:\n",
    "        \"\"\"\n",
    "        :param loss: str, loss function (Available: mse, cross-entropy)\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "\n",
    "    # Mean Squared Error\n",
    "    def mse(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        return (1 / 2) * np.sum((np.linalg.norm(a - y, axis=1)) ** 2)\n",
    "\n",
    "    def d_mse(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        return a - y\n",
    "\n",
    "    # Binary Cross Entropy\n",
    "    def cross_entropy(\n",
    "        self, a: np.ndarray, y: np.ndarray, epsilon: float = 1e-12\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        a = np.clip(a, epsilon, 1.0 - epsilon)\n",
    "        return -np.sum(y * np.log(a))\n",
    "\n",
    "    def d_cross_entropy(\n",
    "        self, a: np.ndarray, y: np.ndarray, epsilon: float = 1e-12\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        a = np.clip(a, epsilon, 1.0 - epsilon)\n",
    "        return -y / a\n",
    "\n",
    "    def get_loss(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            return self.mse(a, y)\n",
    "        elif self.loss == \"cross-entropy\":\n",
    "            return self.cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid losses are mse, cross-entropy\")\n",
    "\n",
    "    def get_d_loss(self, a: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :param a: predicted value\n",
    "        :param y: true value\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            return self.d_mse(a, y)\n",
    "        elif self.loss == \"bce\":\n",
    "            return self.d_bce()\n",
    "        else:\n",
    "            raise ValueError(\"Valid losses are mse, cross-entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6. Learning Rate Decay class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def constant(self, t: int, lr: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        \"\"\"\n",
    "        return lr\n",
    "\n",
    "    def time_decay(self, t: int, lr: float, k: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param k: decay rate\n",
    "        \"\"\"\n",
    "        return lr / (1 + (k * t))\n",
    "\n",
    "    def step_decay(self, t: int, lr: float, F: int, D: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param F: fractor value controlling the decay\n",
    "        :param D: \"Drop every\" iteration\n",
    "        \"\"\"\n",
    "        return lr * (F ** np.floor((1 + t) / D))\n",
    "\n",
    "    def exponential_decay(self, t: int, lr: float, k: float) -> float:\n",
    "        \"\"\"\n",
    "        :param t: iteration number\n",
    "        :param lr: learning rate initial value\n",
    "        :param k: decay rate\n",
    "        \"\"\"\n",
    "        return lr * np.exp(-k * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding2D(BaseLayer):\n",
    "    def __init__(self, p: Union[str, tuple[int, int]] = \"valid\") -> None:\n",
    "        \"\"\"\n",
    "        :param p: padding type (valid, same or tuple[int,int])\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "\n",
    "    def get_dimensions(\n",
    "        self, inp_shape: tuple[int, int], kernel_size: int, s: tuple[int, int] = (1, 1)\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        :param inp_shape: input shape (H,W)\n",
    "        :param kernel_size: kernel size\n",
    "        :param s: stride\n",
    "        \"\"\"\n",
    "        if len(inp_shape) == 4:\n",
    "            m, Nc, Nh, Nw = inp_shape\n",
    "        elif len(inp_shape) == 3:\n",
    "            Nc, Nh, Nw = inp_shape\n",
    "\n",
    "        Kh, Kw = kernel_size\n",
    "        Sh, Sw = s\n",
    "        p = self.p\n",
    "\n",
    "        if type(p) == int:\n",
    "            pt, pb = p, p\n",
    "            pl, pr = p, p\n",
    "        elif type(p) == tuple:\n",
    "            ph, pw = p\n",
    "            pt, pb = ph // 2, (ph + 1) // 2\n",
    "            pl, pr = pw // 2, (pw + 1) // 2\n",
    "        elif p == \"valid\":\n",
    "            pt, pb = 0, 0\n",
    "            pl, pr = 0, 0\n",
    "        elif p == \"same\":\n",
    "            # calculating how much padding is required in all 4 directions\n",
    "            # (top, bottom, left and right)\n",
    "            ph = (Sh - 1) * Nh + Kh - Sh\n",
    "            pw = (Sw - 1) * Nw + Kw - Sw\n",
    "\n",
    "            pt, pb = ph // 2, (ph + 1) // 2\n",
    "            pl, pr = pw // 2, (pw + 1) // 2\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Valid padding types are: valid, same or tuple\")\n",
    "\n",
    "        if len(inp_shape) == 4:\n",
    "            out_shape = (m, Nc, Nh + pt + pb, Nw + pl + pr)\n",
    "        elif len(inp_shape) == 3:\n",
    "            out_shape = (Nc, Nh + pt + pb, Nw + pl + pr)\n",
    "\n",
    "        return out_shape, (pt, pb, pl, pr)\n",
    "\n",
    "    def forward(\n",
    "        self, X: np.ndarray, kernel_size: int, s: tuple[int, int] = (1, 1)\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param kernel_size: kernel size\n",
    "        :param s: stride\n",
    "\n",
    "        :return X_pad: padded input data\n",
    "        \"\"\"\n",
    "\n",
    "        self.inp_shape = X.shape\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "\n",
    "        self.out_shape, (self.pt, self.pb, self.pl, self.pr) = self.get_dimensions(\n",
    "            self.inp_shape, kernel_size, s\n",
    "        )\n",
    "\n",
    "        zeros_r = np.zeros((m, Nc, Nh, self.pr))\n",
    "        zeros_l = np.zeros((m, Nc, Nh, self.pl))\n",
    "        zeros_t = np.zeros((m, Nc, self.pt, Nw + self.pl + self.pr))\n",
    "        zeros_b = np.zeros((m, Nc, self.pb, Nw + self.pl + self.pr))\n",
    "\n",
    "        X_pad = np.concatenate((X, zeros_r), axis=3)\n",
    "        X_pad = np.concatenate((zeros_l, X_pad), axis=3)\n",
    "        X_pad = np.concatenate((zeros_t, X_pad), axis=2)\n",
    "        X_pad = np.concatenate((X_pad, zeros_b), axis=2)\n",
    "\n",
    "        return X_pad\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: Backprop Error of padded X (Xp)\n",
    "\n",
    "        :return dX: Backprop Error of X\n",
    "        \"\"\"\n",
    "        m, Nc, Nh, Nw = self.inp_shape\n",
    "        dX = dZ[:, :, self.pt : self.pt + Nh, self.pl : self.pl + Nw]\n",
    "        return dX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
