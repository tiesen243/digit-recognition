{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Prediction with Convolutional Neural Network\n",
    "\n",
    "- MNIST dataset: is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n",
    "- Goal: build a simple artificial neural network to predict the digit in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib for plotting the images, loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# keras for mnist dataset\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Libs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Base Layer class\n",
    "\n",
    "The base layer class is the parent class of all layers in the network. It has the following methods:\n",
    "- `forward`: forward pass of the layer\n",
    "- `backpropagation`: backward pass of the layer to calculate the gradients and update the weights, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "\n",
    "        TODO: return the output of the layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param dZ: gradient of the loss with respect to the output of the layer\n",
    "        :param lr: learning rate\n",
    "\n",
    "        TODO: update parameters and return the gradient of the input\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Weight Initialization\n",
    "\n",
    "- `zeros`: initialize weights with zeros\n",
    "- `ones`: initialize weights with ones\n",
    "- `random`: initialize weights with random values\n",
    "- `random uniform`: initialize weights with random values from a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    def __init__(self, shape: tuple, init_type: str = \"random\", seed: int = 69) -> None:\n",
    "        \"\"\"\n",
    "        :param shape: shape of the weight matrix\n",
    "        :param init_type: type of the activation function to be used (zeros, ones, random, random_uniform)\n",
    "        :param seed: seed for random number generation\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        self.init_type = init_type\n",
    "        self.seed = seed\n",
    "\n",
    "    def zeros(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a matrix of zeros\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        return np.zeros(shape=self.shape)\n",
    "\n",
    "    def ones(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a matrix of ones\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        return np.ones(shape=self.shape)\n",
    "\n",
    "    def random(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a matrix of random numbers\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        return np.random.randn(*self.shape)\n",
    "\n",
    "    def random_uniform(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a matrix of random numbers from a uniform distribution\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        return np.random.uniform(-1, 1, size=self.shape)\n",
    "\n",
    "    def get(self) -> np.ndarray:\n",
    "        if self.init_type == \"zeros\":\n",
    "            return self.zeros()\n",
    "        elif self.init_type == \"ones\":\n",
    "            return self.ones()\n",
    "        elif self.init_type == \"random\":\n",
    "            return self.random()\n",
    "        elif self.init_type == \"random_uniform\":\n",
    "            return self.random_uniform()\n",
    "        else:\n",
    "            raise ValueError(\"Valid types are zeros, ones, random, random_uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Cost Function\n",
    "\n",
    "- `mean squared error`: mean squared error cost function: $C = \\frac{1}{2n} \\sum_x ||y - a||^2$\n",
    "- `binary cross entropy`: binary cross entropy cost function: $C = -\\frac{1}{n} \\sum_x [y \\ln a + (1 - y) \\ln (1 - a)]$\n",
    "\n",
    "- `derivative of mean squared error`: derivative of the mean squared error cost function: $\\frac{\\partial C}{\\partial a} = 2(a - y) / n$\n",
    "- `derivative of binary cross entropy`: derivative of the binary cross entropy cost function: $\\frac{\\partial C}{\\partial a} = (\\frac{1 - y}{1 - a} - y/a) / n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def __init__(self, cost_type: str = \"mse\") -> None:\n",
    "        \"\"\"\n",
    "        :param cost_type: type of cost function to use (mse, cross_entropy)\n",
    "        \"\"\"\n",
    "        self.cost_type = cost_type\n",
    "\n",
    "    def mse(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param y_true: true values\n",
    "        :param y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def d_mse(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param y_true: true values\n",
    "        :param y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return 2 * (y_pred - y_true) / np.size(y_true)\n",
    "\n",
    "    def cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param y_true: true values\n",
    "        :param y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def d_cross_entropy(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param y_true: true values\n",
    "        :param y_pred: predicted values\n",
    "        \"\"\"\n",
    "        return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "\n",
    "    def get_cost(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        if self.cost_type == \"mse\":\n",
    "            return self.mse(y_true, y_pred)\n",
    "        elif self.cost_type == \"cross_entropy\":\n",
    "            return self.cross_entropy(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost types are mse, cross_entropy\")\n",
    "\n",
    "    def get_d_cost(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        if self.cost_type == \"mse\":\n",
    "            return self.d_mse(y_true, y_pred)\n",
    "        elif self.cost_type == \"cross_entropy\":\n",
    "            return self.d_cross_entropy(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost types are mse, cross_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4. Plotting function\n",
    "The plotting functions is used to plot the images and its true labels and predicted labels (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_proba: np.ndarray = None,\n",
    "    shape: tuple[int, int] = (2, 4),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    :param X: input data (images)\n",
    "    :param y: true labels\n",
    "    :param y_proba: predicted labels\n",
    "    :param shape: shape of the plot (nrows, ncols)\n",
    "    \"\"\"\n",
    "    nrows, ncols = shape\n",
    "    _, axes = plt.subplots(nrows, ncols, figsize=(10, 5))\n",
    "\n",
    "    len_x = X.shape[0]\n",
    "    for idx in range(nrows * ncols):\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "        img_idx = np.random.randint(0, len_x)\n",
    "\n",
    "        ax.imshow(X[img_idx], cmap=\"gray\")\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "\n",
    "        true_label = f\"True: {y[img_idx]}\"\n",
    "        color = \"black\"\n",
    "\n",
    "        if y_proba is not None:\n",
    "            pred_label = f\"Pred: {y_proba[img_idx]}\"\n",
    "            color = \"green\" if y[img_idx] == y_proba[img_idx] else \"red\"\n",
    "\n",
    "        img_title = true_label if y_proba is None else f\"{true_label}\\n{pred_label}\"\n",
    "        ax.set_xlabel(img_title, color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Convolutional Layer\n",
    "\n",
    "The convolutional layer has the following methods:\n",
    "\n",
    "- `get_roi`: get the region of interest (ROI) of the input image\n",
    "- `forward`: forward pass of the convolutional layer to calculate the output feature map: $z = \\sum_{i=0}^{n} W_i \\cdot X_i + b$\n",
    "- `backpropagation`: backward pass of the convolutional layer to calculate the gradients and update kernels and bias: \n",
    "\n",
    "$\\frac{\\partial C}{\\partial W_i} = \\sum_{j} \\frac{\\partial C}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial W_i}$ and $\\frac{\\partial C}{\\partial b} = \\sum_{j} \\frac{\\partial C}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(BaseLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_shape: tuple,\n",
    "        kernel_size: int = 3,\n",
    "        depth: int = 1,\n",
    "        p: int = 0,\n",
    "        s: int = 1,\n",
    "        init_type: str = \"random\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param inp_shape: shape of the input data\n",
    "        :param kernel_size: size of the kernel\n",
    "        :param depth: depth of the output data\n",
    "        :param p: padding\n",
    "        :param s: stride\n",
    "        :param w_init_type: type of weight initialization\n",
    "        \"\"\"\n",
    "        inp_depth, inp_height, inp_width = inp_shape\n",
    "        self.inp_shape = inp_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth = depth\n",
    "        self.p = p\n",
    "        self.s = s\n",
    "\n",
    "        self.out_shape = (\n",
    "            depth,\n",
    "            (inp_height - kernel_size + 2 * p) // s + 1,\n",
    "            (inp_width - kernel_size + 2 * p) // s + 1,\n",
    "        )\n",
    "        self.kernels_shape = (depth, kernel_size, kernel_size)\n",
    "        self.kernels = WeightInitializer(\n",
    "            shape=self.kernels_shape, init_type=init_type\n",
    "        ).get()\n",
    "        self.bias = WeightInitializer(shape=self.out_shape, init_type=init_type).get()\n",
    "\n",
    "    def get_roi(self, X: np.ndarray):\n",
    "        for row in range(self.out_shape[1]):\n",
    "            for col in range(self.out_shape[2]):\n",
    "                yield row, col, X[\n",
    "                    :, row : row + self.kernel_size, col : col + self.kernel_size\n",
    "                ]\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "        self.output = np.copy(self.bias)\n",
    "\n",
    "        for row, col, roi in self.get_roi(X):\n",
    "            self.output[:, row, col] = np.sum(roi * self.kernels, axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        dK = np.zeros(self.kernels_shape)\n",
    "        dX = np.zeros(self.inp_shape)\n",
    "\n",
    "        for row, col, roi in self.get_roi(self.X):\n",
    "            dK += dZ[:, row, col].reshape(-1, 1, 1) * roi\n",
    "            dX[:, row : row + self.kernel_size, col : col + self.kernel_size] += np.sum(\n",
    "                self.kernels * dZ[:, row, col].reshape(-1, 1, 1), axis=0\n",
    "            )\n",
    "\n",
    "        self.kernels -= lr * dK\n",
    "        self.bias -= lr * dZ\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Max Pooling Layer\n",
    "\n",
    "Using max pooling to reduce the size of the feature map and keep the most important information.\n",
    "\n",
    "The max pooling layer has the following methods:\n",
    "\n",
    "- `get_roi`: get the region of interest (ROI) of the input image\n",
    "- `forward`: forward pass of the max pooling layer to calculate the output feature map: $z = \\max(X_i)$\n",
    "- `backpropagation`: backward pass of the max pooling layer to calculate the gradients: $\\frac{\\partial C}{\\partial X_i} = \\frac{\\partial C}{\\partial z} \\cdot \\frac{\\partial z}{\\partial X_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling2D(BaseLayer):\n",
    "    def __init__(self, inp_shape: tuple, pool_size: int = 2) -> None:\n",
    "        \"\"\"\n",
    "        :param inp_shape: shape of the input data\n",
    "        :param pool_size: size of the pooling\n",
    "        \"\"\"\n",
    "        inp_depth, inp_height, inp_width = inp_shape\n",
    "        self.inp_shape = inp_shape\n",
    "        self.pool_size = pool_size\n",
    "        self.s = 2\n",
    "\n",
    "        self.out_shape = (\n",
    "            inp_depth,\n",
    "            (inp_height - pool_size) // self.s + 1,\n",
    "            (inp_width - pool_size) // self.s + 1,\n",
    "        )\n",
    "\n",
    "    def get_roi(self, X: np.ndarray):\n",
    "        for row in range(0, self.inp_shape[1] - self.pool_size + 1, self.s):\n",
    "            for col in range(0, self.inp_shape[2] - self.pool_size + 1, self.s):\n",
    "                yield row, col, X[\n",
    "                    :, row : row + self.pool_size, col : col + self.pool_size\n",
    "                ]\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "        self.output = WeightInitializer(shape=self.out_shape, init_type=\"zeros\").get()\n",
    "\n",
    "        for row, col, roi in self.get_roi(X):\n",
    "            self.output[:, row // self.s, col // self.s] = np.max(roi, axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        dX = WeightInitializer(shape=self.inp_shape, init_type=\"zeros\").get()\n",
    "\n",
    "        for row, col, roi in self.get_roi(self.X):\n",
    "            mask = np.max(roi, axis=(1, 2), keepdims=True) == roi\n",
    "            dX[:, row : row + self.pool_size, col : col + self.pool_size] += (\n",
    "                mask * dZ[:, row // self.s, col // self.s][:, None, None]\n",
    "            )\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Flatten Layer class\n",
    "\n",
    "Unroll the 2D image to a 1D array.\n",
    "\n",
    "The flatten layer has the following methods:\n",
    "\n",
    "- `forward`: forward pass of the flatten layer to flatten the input image to a 1D array\n",
    "- `backpropagation`: backward pass of the flatten layer to reshape the 1D array to the original shape of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(BaseLayer):\n",
    "    def __init__(self, inp_shape: tuple) -> None:\n",
    "        \"\"\"\n",
    "        :param inp_shape: shape of the input data\n",
    "        \"\"\"\n",
    "        self.inp_shape = inp_shape\n",
    "        self.out_shape = (np.prod(inp_shape), 1)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(X, self.out_shape)\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        return np.reshape(dZ, self.inp_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4. Dropout Layer class\n",
    "\n",
    "Using dropout is a technique to prevent overfitting by randomly setting some neurons to zero during training.\n",
    "\n",
    "The dropout layer has the following methods:\n",
    "\n",
    "- `forward`: forward pass of the dropout layer to randomly set some neurons to zero\n",
    "- `backpropagation`: backward pass of the dropout layer to set the gradients of the dropped neurons to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(BaseLayer):\n",
    "    def __init__(self, p: float = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        :param p: probability of dropout\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.mask = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        return X * self.mask\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        return dZ * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5. Dense Layer class\n",
    "\n",
    "Using the dense layer to connect the convolutional and max pooling layers to the output layer.\n",
    "\n",
    "The dense layer has the following methods:\n",
    "\n",
    "- `forward`: forward pass of the dense layer to calculate the output: $z = w \\cdot a + b$\n",
    "- `backpropagation`: backward pass of the dense layer to calculate the gradients and update weights and bias: $\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z} \\cdot a^T$, $\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(BaseLayer):\n",
    "    def __init__(self, inp_size: int, out_size: int, init_type: str = \"random\") -> None:\n",
    "        \"\"\"\n",
    "        :param inp_size: input size of the layer\n",
    "        :param out_size: output size of the layer\n",
    "        :param init_type: type of weight initialization (zeros, ones, random, random_uniform)\n",
    "        \"\"\"\n",
    "        self.weights = WeightInitializer(\n",
    "            shape=(out_size, inp_size), init_type=\"random\"\n",
    "        ).get()\n",
    "        self.bias = WeightInitializer(shape=(out_size, 1), init_type=\"random\").get()\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "        return np.dot(self.weights, self.X) + self.bias\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        dW = np.dot(dZ, self.X.T)\n",
    "        self.weights -= lr * dW\n",
    "        self.bias -= lr * dZ\n",
    "        return np.dot(self.weights.T, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6. Activation Layer class\n",
    "\n",
    "Use the activation layer to apply an activation function to the output of the previous layer. The activation layer has the following methods:\n",
    "\n",
    "- `linear`: linear activation function $f(x) = x$\n",
    "- `reLU`: rectified linear unit activation function $f(x) = max(0, x)$\n",
    "- `sigmoid`: sigmoid activation function $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- `tanh`: hyperbolic tangent activation function $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "- `derivative of linear`: derivative of the linear activation function $f'(x) = 1$\n",
    "- `derivative of reLU`: derivative of the reLU activation function $f'(x) = 1$ if $x > 0$ and $0$ otherwise\n",
    "- `derivative of sigmoid`: derivative of the sigmoid activation function $f'(x) = sigmoid(x) \\cdot (1 - sigmoid(x))$\n",
    "- `derivative of tanh`: derivative of the tanh activation function $f'(x) = 1 - tanh(x)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(BaseLayer):\n",
    "    def __init__(self, act_type: str = \"reLU\") -> None:\n",
    "        \"\"\"\n",
    "        :param act_type: type of the activation function to be used (linear, reLU, sigmoid, tanh)\n",
    "        \"\"\"\n",
    "        self.act_type = act_type\n",
    "\n",
    "    def linear(self, X: np.ndarray) -> np.ndarray:\n",
    "        return X\n",
    "\n",
    "    def d_linear(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1\n",
    "\n",
    "    def reLU(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def d_reLU(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def d_sigmoid(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "\n",
    "    def tanh(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def d_tanh(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1 - np.tanh(X) ** 2\n",
    "\n",
    "    def get_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.act_type == \"linear\":\n",
    "            return self.linear(X)\n",
    "        elif self.act_type == \"reLU\":\n",
    "            return self.reLU(X)\n",
    "        elif self.act_type == \"sigmoid\":\n",
    "            return self.sigmoid(X)\n",
    "        elif self.act_type == \"tanh\":\n",
    "            return self.tanh(X)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type\")\n",
    "\n",
    "    def get_d_activation(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.act_type == \"linear\":\n",
    "            return self.d_linear(X)\n",
    "        elif self.act_type == \"reLU\":\n",
    "            return self.d_reLU(X)\n",
    "        elif self.act_type == \"sigmoid\":\n",
    "            return self.d_sigmoid(X)\n",
    "        elif self.act_type == \"tanh\":\n",
    "            return self.d_tanh(X)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type\")\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "        return self.get_activation(self.X)\n",
    "\n",
    "    def backpropagation(self, dZ: np.ndarray, lr: float) -> np.ndarray:\n",
    "        return np.multiply(dZ, self.get_d_activation(self.X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Convolutional Neural Network (CNN) Model class\n",
    "\n",
    "The CNN model class has the following methods:\n",
    "\n",
    "- `summary`: print the summary of the model including the layers and the number of parameters\n",
    "- `fit`: train the model using the training data and labels\n",
    "- `plot_history`: plot the training history including the loss and accuracy\n",
    "- `predict`: predict the labels of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, layers: list[BaseLayer]) -> None:\n",
    "        if not layers:\n",
    "            raise ValueError(\"No layers provided\")\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "        # Initialize the network architecture\n",
    "        self.layer_name = []\n",
    "        self.architecture = []\n",
    "\n",
    "    def create_network_architecture(self):\n",
    "        for layer in self.layers:\n",
    "            layer_name = layer.__class__.__name__\n",
    "            if layer_name in [\"Conv2D\", \"Pooling2D\", \"Flatten\"]:\n",
    "                self.layer_name.append(layer_name)\n",
    "                if layer_name == \"Flatten\":\n",
    "                    self.architecture.append(layer.out_shape[:-1])\n",
    "                else:\n",
    "                    self.architecture.append(layer.out_shape)\n",
    "            elif layer_name == \"Dense\":\n",
    "                self.layer_name.append(\"Dense\")\n",
    "                self.architecture.append(layer.weights.shape[:-1])\n",
    "            elif layer_name == \"Dropout\":\n",
    "                self.layer_name.append(\"Dropout\")\n",
    "                self.architecture.append(self.architecture[-1])\n",
    "            elif layer_name == \"Activation\":\n",
    "                continue\n",
    "\n",
    "    def compile(self, cost_type: str, lr: float, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        :param cost_type: type of cost function to use (mse, cross_entropy)\n",
    "        :param lr: learning rate\n",
    "        :param verbose: whether to print the cost and accuracy after each epoch\n",
    "        \"\"\"\n",
    "        self.cost = Cost(cost_type)\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def summary(self) -> None:\n",
    "        self.create_network_architecture()\n",
    "        len_assigned = [45, 26, 15]\n",
    "        count = {\n",
    "            \"Conv2D\": 1,\n",
    "            \"Pooling2D\": 1,\n",
    "            \"Flatten\": 1,\n",
    "            \"Dropout\": 1,\n",
    "            \"Dense\": 1,\n",
    "            \"Activation\": 1,\n",
    "        }\n",
    "        col_names = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n",
    "        print(\"Model: CNN\")\n",
    "        print(\"-\" * sum(len_assigned))\n",
    "\n",
    "        text = \"\"\n",
    "        for i in range(3):\n",
    "            text += col_names[i] + \" \" * (len_assigned[i] - len(col_names[i]))\n",
    "        print(text)\n",
    "        print(\"=\" * sum(len_assigned))\n",
    "\n",
    "        total_params = 0\n",
    "        for i in range(len(self.layer_name)):\n",
    "            layer_name = self.layer_name[i]\n",
    "            name = f\"{layer_name.lower()}_{count[layer_name]} ({layer_name})\"\n",
    "\n",
    "            count[layer_name] += 1\n",
    "            if self.architecture[i] is None:\n",
    "                print(\n",
    "                    f\"{name}{' ' * (len_assigned[0] - len(name))}{' ' * (len_assigned[1] - 2)}{' ' * (len_assigned[2] - 2)}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            total_params += np.prod(self.architecture[i])\n",
    "            output_shape = self.architecture[i]\n",
    "            output_shape = f\"(None, {', '.join(map(str, output_shape))})\"\n",
    "            params = np.prod(self.architecture[i])\n",
    "\n",
    "            print(\n",
    "                f\"{name}{' ' * (len_assigned[0] - len(name))}{output_shape}{' ' * (len_assigned[1] - len(output_shape))}{params}\"\n",
    "            )\n",
    "\n",
    "        print(\"=\" * sum(len_assigned))\n",
    "\n",
    "        print(f\"Total params: {total_params}\")\n",
    "        print(f\"Cost function: {self.cost.cost_type}\")\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Verbose: {self.verbose}\")\n",
    "        print(\"-\" * sum(len_assigned))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        :param X: input data\n",
    "        :param y: true labels\n",
    "        :param epochs: number of epochs to train the model\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the history dictionary\n",
    "        self.history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize the error and correct predictions\n",
    "            error = 0\n",
    "            correct = 0\n",
    "\n",
    "            for i in tqdm(\n",
    "                range(len(X)),\n",
    "                colour=\"GREEN\",\n",
    "                ascii=\"░▒█\",\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "            ):\n",
    "                output = X[i]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                error += self.cost.get_cost(y[i], output)\n",
    "                correct += int(np.argmax(output) == np.argmax(y[i]))\n",
    "\n",
    "                dZ = self.cost.get_d_cost(y[i], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    dZ = layer.backpropagation(dZ, self.lr)\n",
    "\n",
    "            self.history[\"loss\"].append(error / len(X))\n",
    "            self.history[\"accuracy\"].append(correct / len(X))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"Loss: {self.history['loss'][-1]:.4f}, Accuracy: {self.history['accuracy'][-1]:.4f}\"\n",
    "                )\n",
    "\n",
    "    def plot_history(self) -> None:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        ax[0].plot(self.history[\"loss\"])\n",
    "        ax[0].set_title(\"Loss\")\n",
    "        ax[0].set_xlabel(\"Epoch\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "        ax[1].plot(self.history[\"accuracy\"])\n",
    "        ax[1].set_title(\"Accuracy\")\n",
    "        ax[1].set_xlabel(\"Epoch\")\n",
    "        ax[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            output = X[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            predictions.append(output)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training the model with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Preprocessing the data\n",
    "\n",
    "- Load the MNIST dataset\n",
    "- Normalize the data by reshaping the images to 1x28x28 and dividing by 255 to scale the pixel values to the range [0, 1]\n",
    "- One-hot encode the labels to convert the labels to a binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "plot_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train.reshape(len(X_train), 1, 28, 28)\n",
    "X_test = X_test.reshape(len(X_test), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Reshape the labels\n",
    "y_train = y_train.reshape(len(y_train), 10, 1)\n",
    "y_test = y_test.reshape(len(y_test), 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    [\n",
    "        Conv2D(\n",
    "            (1, 28, 28), kernel_size=3, depth=5, p=2, s=2, init_type=\"random\"\n",
    "        ),  # output shape =(input_size - kernel_size + 2 * p) / s + 1 = (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "        Activation(act_type=\"sigmoid\"),\n",
    "        Pooling2D(\n",
    "            (5, 15, 15), pool_size=2\n",
    "        ),  # output shape = (input_size - pool_size) / s + 1 = (15 - 2) / 2 + 1 = 7\n",
    "        Flatten((5, 7, 7)),\n",
    "        Dropout(p=0.4),\n",
    "        Dense(5 * 7 * 7, 100),\n",
    "        Activation(act_type=\"sigmoid\"),\n",
    "        Dense(100, 10),\n",
    "        Activation(act_type=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(cost_type=\"cross_entropy\", lr=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3. Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(\n",
    "    X_test.reshape(len(X_test), 28, 28),\n",
    "    np.argmax(y_test, axis=1),\n",
    "    np.argmax(y_pred, axis=1),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
